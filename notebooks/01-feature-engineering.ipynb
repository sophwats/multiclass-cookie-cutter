{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we transform the data into feature vectors. We want the feature vectors to capture information about a message so that we can go on to train a model which can classify messages from the 20 catagories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_parquet(\"./data/training.parquet\")\n",
    "training_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains two columns - one holding the message and one for the category to which that message belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform each of the messages into a numeric vector. One way to do this would be to associate a space in a bit vector with every possible word. We could set each bit to `True` or `1` if the corresponding word appears in that message, and `0` or `False` otherwise. In practice this is not practical because of the huge memory requirements. Instead we use a 'hashing vectoriser' which _hashes_ the words to vectors of a fixed, finite number of bits. \n",
    "\n",
    "You can see that many of the messages contain non-standard words or character strings such as email addresses. It is common practice to ignore such words during the feature engineering stage. We do this by setting a 'token_pattern' parameter to ensure that only standard words are hashed. \n",
    "\n",
    "We also want to remove any 'stop words' from the text. We do this by setting `stop_words='english'`.\n",
    "\n",
    "✅ Try training models with and without setting the `token_pattern` parameter. How does the model performance changes? What about including stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "BUCKETS=2048\n",
    "\n",
    "hv = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', stop_words ='english', n_features=BUCKETS, alternate_sign = False)\n",
    "hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvcounts = hv.fit_transform(training_data[\"Text\"])\n",
    "hvcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 13,947 messages in the training set is now represented by one vector of length 'BUCKETS'. \n",
    "\n",
    "\n",
    "✅ We fix the number of buckets at 2,048 by default, but you can try using a different number for BUCKETS and see how the classification models are affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - inverse document frequency (TF-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use term frequency-inverse document frequency, or tf-idf, to generate feature vectors. Tf-idf is commonly used to summarise text data, and it aims to capture how important different words are within a set of documents. Tf-idf combines a normalized word count (or term frequency) with the inverse document frequency (or a measure of how common a word is across all documents) in order to identify words, or terms, which are 'interesting' within the document, with respect to the set of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "hvdf_tfidf = tfidf_transformer.fit_transform(hvcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdf_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to visualise your data at this point - we want to visualise the feature vectors to see if we can identify any clustering or structure in them which can be used to distinguish between messages belonging to different categories.\n",
    "\n",
    "The vectors are 2048 dimensions (unless you changed the `BUCKETS` parameter), so it's not possible to plot them as they are! Instead we will use Principal Component Analysis (PCA) to project the data down to two dimensions, which we can then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "DIMENSIONS = 2\n",
    "\n",
    "pca2 = sklearn.decomposition.TruncatedSVD(DIMENSIONS)\n",
    "pca_a = pca2.fit_transform(hvdf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_a, columns=[\"x\", \"y\"])\n",
    "pca_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([training_data.reset_index(), pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "alt.Chart(plot_data.sample(2000)).encode(x=\"x\", y=\"y\", color=\"Category\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows no clear clustering within classes. However, this is expected for a couple of reasons:\n",
    "   1. We are mapping from a 2048 dimensional space down to 2 dimensions. This is a huge reduction in dimensions and as such we lose some information.\n",
    "   2.  We are plotting 20 different categories on one plot - it's difficult to see what's going on clearly with all those colours. \n",
    "    \n",
    "Instead we want to plot each category individually against all other categories. This one-vs-all plot may enable us to see any clusters more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair.expr import datum\n",
    "\n",
    "data=plot_data.sample(2000)\n",
    "\n",
    "base = alt.Chart(data).mark_point().encode(\n",
    "    alt.X('x', scale=alt.Scale(domain=(0, 0.6))),\n",
    "    alt.Y('y', scale = alt.Scale(domain = (-0.3, 0.3))),\n",
    "    color='Category:N'\n",
    ")\n",
    "\n",
    "## Make gray points for background\n",
    "chart1 = alt.Chart(data.sample(1000)) \\\n",
    "            .encode(x=\"x\", y=\"y\") \\\n",
    "            .mark_point(opacity=0.3, color=\"lightgray\")\n",
    "\n",
    "\n",
    "def overlay_make_chart(base_chart, category, options, chart1):\n",
    "    title = category\n",
    "    chart2 = base_chart\\\n",
    "      .transform_filter(datum.Category == category)\\\n",
    "      .properties(width=options['width'], \n",
    "                    height=options['height'], title=title)\n",
    "    chart = chart1 + chart2\n",
    "    return chart\n",
    "\n",
    "\n",
    "options = {'width': 150, 'height': 150}\n",
    "charts = [overlay_make_chart(base, category, options, chart1) \\\n",
    "          for category in sorted(data.Category.unique())]\n",
    "\n",
    "# make a single row\n",
    "def make_chart_row(row_of_charts):\n",
    "    hconcat = [chart for chart in row_of_charts]\n",
    "    row = alt.HConcatChart(hconcat=hconcat)\n",
    "    return row\n",
    "\n",
    "# take an array of charts and produce a facet grid\n",
    "def facet_wrap(charts, charts_per_row):\n",
    "    rows_of_charts = [\n",
    "        charts[i:i+charts_per_row] \n",
    "        for i in range(0, len(charts), charts_per_row)]        \n",
    "    vconcat = [make_chart_row(r) for r in rows_of_charts]    \n",
    "    vcc = alt.VConcatChart(vconcat=vconcat)\\\n",
    "      .configure_axisX(grid=True)\\\n",
    "      .configure_axisY(grid=True)\n",
    "    return vcc\n",
    "\n",
    "# assemble the facet grid\n",
    "compound_chart = facet_wrap(charts, charts_per_row=3)\n",
    "compound_chart.properties(title='PCA projections, by Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This facet plot shows that the PCA projections are clustering by category, thus indicating that our feature engineering approach is able to identify structure in our data. \n",
    "\n",
    "As such, we are happy with our feature engineering technique and will save the transformation steps as a pipeline stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "BUCKETS=2048\n",
    "hv = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', stop_words ='english', n_features=BUCKETS, alternate_sign = False)\n",
    "tfidf = TfidfTransformer()\n",
    "feat_pipeline = Pipeline([('vect',hv), ('tfidf',tfidf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "\n",
    "util.serialize_to(feat_pipeline, \"feature_pipeline.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
